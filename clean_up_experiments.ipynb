{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Tuple, List, Type\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.functional import cross_entropy\n",
    "from torch.utils.data import random_split, DataLoader\n",
    "from tqdm import tqdm\n",
    "from torch.functional import F\n",
    "\n",
    "from src.metrics import get_model_flops\n",
    "from src.reference import ReferenceModel\n",
    "from src.data import RotatedMNISTDataset, FixedSizeWrapper\n",
    "from src.binary_tree import MNISTOracleRouter, BinaryTreeGoE, LatentVariableRouter, RandomBinaryTreeRouter\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_modules_depth4(param_factor: int) -> Tuple[Type[nn.Module], Type[nn.Module], Type[nn.Module]]:\n",
    "    \"\"\"\n",
    "    Architecture factory for RotatedMNIST task. Takes in hyperparameter param_factor, outputs list of module architectures which scale linearly in param_factor\n",
    "    \"\"\"\n",
    "    class LayerOne(nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "            self.conv = nn.Conv2d(1, param_factor, kernel_size=3, padding=1)\n",
    "    \n",
    "        def forward(self, x: torch.Tensor):\n",
    "            \"\"\"\n",
    "            Args:\n",
    "            - x: (batch_size, 1, 32, 32)\n",
    "            \"\"\"\n",
    "            return F.relu(F.max_pool2d(self.conv(x), 2))  # (batch_size, param_factor, 16, 16)\n",
    "    \n",
    "    class LayerTwo(nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "            self.conv = nn.Conv2d(param_factor, 2 * param_factor, kernel_size=3, padding=1)\n",
    "    \n",
    "        def forward(self, x: torch.Tensor):\n",
    "            \"\"\"\n",
    "            Args:\n",
    "            - x: (batch_size, 32, 16, 16)\n",
    "            \"\"\"\n",
    "            return F.relu(F.max_pool2d(self.conv(x), 2))  # (batch_size, 2 * param_factor, 8, 8)\n",
    "\n",
    "    class LayerThree(nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "            self.conv = nn.Conv2d(2 * param_factor, 4 * param_factor, kernel_size=3, padding=1)\n",
    "    \n",
    "        def forward(self, x: torch.Tensor):\n",
    "            \"\"\"\n",
    "            Args:\n",
    "            - x: (batch_size, 64, 8, 8)\n",
    "            \"\"\"\n",
    "            return F.relu(F.max_pool2d(self.conv(x), 2))  # (batch_size, 4 * param_factor, 4, 4)\n",
    "    \n",
    "    class LayerFour(nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "            self.fc1 = nn.Linear(4 * param_factor * 7 * 7, 8 * param_factor)\n",
    "            self.fc2 = nn.Linear(8 * param_factor, 10)\n",
    "    \n",
    "        def forward(self, x: torch.Tensor):\n",
    "            \"\"\"\n",
    "            Args:\n",
    "            - x: (batch_size, 128, 4, 4)\n",
    "            \"\"\"\n",
    "            x = x.view(-1, 4 * param_factor * 4 * 4)  # (batch_size, 4 * param_factor * 4 * 4)\n",
    "            x = F.relu(self.fc1(x))  # (batch_size, 8 * param_factor)\n",
    "            logits = self.fc2(x)  # (batch_size, 10)\n",
    "            return logits\n",
    "    return LayerOne, LayerTwo, LayerThree, LayerFour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_modules_sqA(param_factor: int) -> Tuple[Type[nn.Module], Type[nn.Module], Type[nn.Module]]:\n",
    "    \"\"\"\n",
    "    Architecture factory for Status Quo A Model for RotatedMNIST task which has the same number of *total* parameters as the Graph of Experts. Takes in hyperparameter param_factor, outputs list of module architectures which scale linearly in param_factor\n",
    "    \"\"\"\n",
    "    class LayerOne(nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "            self.conv = nn.Conv2d(1, param_factor, kernel_size=3, padding=1)\n",
    "    \n",
    "        def forward(self, x: torch.Tensor):\n",
    "            \"\"\"\n",
    "            Args:\n",
    "            - x: (batch_size, 1, 28, 28)\n",
    "            \"\"\"\n",
    "            return F.relu(F.max_pool2d(self.conv(x), 2))  # (batch_size, param_factor, 14, 14)\n",
    "    \n",
    "    class LayerTwo(nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "            self.conv = nn.Conv2d(param_factor, 2 * 2 * param_factor, kernel_size=3, padding=1)\n",
    "    \n",
    "        def forward(self, x: torch.Tensor):\n",
    "            \"\"\"\n",
    "            Args:\n",
    "            - x: (batch_size, 32, 14, 14)\n",
    "            \"\"\"\n",
    "            return F.relu(F.max_pool2d(self.conv(x), 2))  # (batch_size, 2 * 2 * param_factor, 7, 7)\n",
    "    \n",
    "    class LayerThree(nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "            self.fc1 = nn.Linear(2 * 2 * param_factor * 7 * 7, 2 * 4 * param_factor)\n",
    "            self.fc2 = nn.Linear(2 * 4 * param_factor, 10)\n",
    "    \n",
    "        def forward(self, x: torch.Tensor):\n",
    "            \"\"\"\n",
    "            Args:\n",
    "            - x: (batch_size, 128, 7, 7)\n",
    "            \"\"\"\n",
    "            x = x.view(-1, 2 * 2 * param_factor * 7 * 7)  # (batch_size, 2 * 2 * param_factor * 7 * 7)\n",
    "            x = F.relu(self.fc1(x))  # (batch_size, 2 * 4 * param_factor)\n",
    "            logits = self.fc2(x)  # (batch_size, 10)\n",
    "            return logits\n",
    "    return LayerOne, LayerTwo, LayerThree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_modules(param_factor: int) -> Tuple[Type[nn.Module], Type[nn.Module], Type[nn.Module]]:\n",
    "    \"\"\"\n",
    "    Architecture factory for RotatedMNIST task. Takes in hyperparameter param_factor, outputs list of module architectures which scale linearly in param_factor\n",
    "    \"\"\"\n",
    "    class LayerOne(nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "            self.conv = nn.Conv2d(1, param_factor, kernel_size=3, padding=1)\n",
    "    \n",
    "        def forward(self, x: torch.Tensor):\n",
    "            \"\"\"\n",
    "            Args:\n",
    "            - x: (batch_size, 1, 28, 28)\n",
    "            \"\"\"\n",
    "            return F.relu(F.max_pool2d(self.conv(x), 2))  # (batch_size, param_factor, 14, 14)\n",
    "    \n",
    "    class LayerTwo(nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "            self.conv = nn.Conv2d(param_factor, 2 * param_factor, kernel_size=3, padding=1)\n",
    "    \n",
    "        def forward(self, x: torch.Tensor):\n",
    "            \"\"\"\n",
    "            Args:\n",
    "            - x: (batch_size, 32, 14, 14)\n",
    "            \"\"\"\n",
    "            return F.relu(F.max_pool2d(self.conv(x), 2))  # (batch_size, 2 * param_factor, 7, 7)\n",
    "    \n",
    "    class LayerThree(nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "            self.fc1 = nn.Linear(2 * param_factor * 7 * 7, 4 * param_factor)\n",
    "            self.fc2 = nn.Linear(4 * param_factor, 10)\n",
    "    \n",
    "        def forward(self, x: torch.Tensor):\n",
    "            \"\"\"\n",
    "            Args:\n",
    "            - x: (batch_size, 64, 7, 7)\n",
    "            \"\"\"\n",
    "            x = x.view(-1, 2 * param_factor * 7 * 7)  # (batch_size, 2 * param_factor * 7 * 7)\n",
    "            x = F.relu(self.fc1(x))  # (batch_size, 4 * param_factor)\n",
    "            logits = self.fc2(x)  # (batch_size, 10)\n",
    "            return logits\n",
    "    return LayerOne, LayerTwo, LayerThree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def do_train_epoch(model, loader, optimizer, epoch) -> float:\n",
    "    model.train()\n",
    "    epoch_losses = []\n",
    "    for idx, (images, rotation_labels, digit_labels) in enumerate(loader):\n",
    "        images = images.to(device)\n",
    "        rotation_labels = rotation_labels.to(device)\n",
    "        digit_labels = digit_labels.to(device)\n",
    "\n",
    "        router_metadata = {\n",
    "            \"rotation_labels\": rotation_labels,\n",
    "        }\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(images, router_metadata=router_metadata) # (bs, num_digit_classes)\n",
    "        loss = cross_entropy(logits, digit_labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_losses.append(loss.item())\n",
    "    epoch_loss = torch.mean(torch.Tensor(epoch_losses)).item()\n",
    "    # print(f'Train epoch {epoch} loss: {epoch_loss:.4f}')\n",
    "    return epoch_loss\n",
    "\n",
    "def do_val_epoch(model, loader, epoch) -> float:\n",
    "    model.eval()\n",
    "    epoch_losses = []\n",
    "    for idx, (images, rotation_labels, digit_labels) in tqdm(enumerate(loader)):\n",
    "        images = images.to(device)\n",
    "        rotation_labels = rotation_labels.to(device)\n",
    "        digit_labels = digit_labels.to(device)\n",
    "        \n",
    "        router_metadata = {\n",
    "            \"rotation_labels\": rotation_labels,\n",
    "        }\n",
    "\n",
    "        logits = model(images, router_metadata=router_metadata) # (bs, num_digit_classes)\n",
    "        loss = cross_entropy(logits, digit_labels)\n",
    "        epoch_losses.append(loss.item())\n",
    "    epoch_loss = torch.mean(torch.Tensor(epoch_losses)).item()\n",
    "    # print(f'Val epoch {epoch} loss: {epoch_loss:.4f}') \n",
    "    return epoch_loss\n",
    "\n",
    "def get_accuracy(model, loader):\n",
    "    model.eval()\n",
    "    total_samples = 0\n",
    "    total_correct = 0\n",
    "    for idx, (images, rotation_labels, digit_labels) in enumerate(loader):\n",
    "        images = images.to(device)\n",
    "        rotation_labels = rotation_labels.to(device)\n",
    "        digit_labels = digit_labels.to(device) # (bs,)\n",
    "        \n",
    "        router_metadata = {\n",
    "            \"rotation_labels\": rotation_labels,\n",
    "        }\n",
    "\n",
    "        logits = model(images, router_metadata=router_metadata) # (bs, num_digit_classes)\n",
    "        prediction = torch.argmax(logits, dim=1) # (bs,)\n",
    "        total_samples += prediction.shape[0]\n",
    "        total_correct += torch.sum(prediction == digit_labels).item()\n",
    "    accuracy = total_correct / total_samples * 100\n",
    "    return accuracy\n",
    "\n",
    "def get_rotated_mnist_loaders(downsample_factor: int = 20) -> Tuple[DataLoader, DataLoader, DataLoader]:\n",
    "    # Initialize dataset\n",
    "    dataset = RotatedMNISTDataset()\n",
    "    \n",
    "    # Assuming `dataset` is your PyTorch Dataset\n",
    "    dataset_size = len(dataset)\n",
    "    train_size = int(0.7 * dataset_size)\n",
    "    val_size = int(0.2 * dataset_size)\n",
    "    test_size = dataset_size - train_size - val_size\n",
    "    \n",
    "    train_dataset, val_dataset, test_dataset = random_split(\n",
    "        dataset, [train_size, val_size, test_size], \n",
    "        generator=torch.Generator().manual_seed(40)\n",
    "    )\n",
    "    \n",
    "    train_dataset = FixedSizeWrapper(dataset = train_dataset, size = train_size // downsample_factor)\n",
    "    val_dataset = FixedSizeWrapper(dataset = val_dataset, size = val_size // downsample_factor)\n",
    "    test_dataset = FixedSizeWrapper(dataset = test_dataset, size = test_size // downsample_factor)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=False)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "    return train_dataset, val_dataset, test_dataset, train_loader, val_loader, test_loader\n",
    "\n",
    "def training_loop(model: nn.Module, train_loader: DataLoader, val_loader: DataLoader, test_loader: DataLoader, num_epochs: int = 100, lr: float = 0.005, epochs_per_accuracy: int = 5) -> List[float]:\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    val_losses = []\n",
    "    accuracy = None\n",
    "    # make a pbar displaying the epoch number, training loss, and validation loss, and test accuracy\n",
    "    pbar = tqdm(range(num_epochs), desc='Epochs', total=num_epochs, leave=False, position=0)\n",
    "    \n",
    "    for epoch in pbar:\n",
    "        train_loss = do_train_epoch(model, train_loader, optimizer, epoch)\n",
    "        val_loss = do_val_epoch(model, val_loader, epoch)\n",
    "        val_losses.append(val_loss)\n",
    "        if epoch % epochs_per_accuracy == 0:\n",
    "            accuracy = get_accuracy(model, test_loader)\n",
    "    \n",
    "        pbar.set_postfix({'Train loss': train_loss, 'Val loss': val_loss, 'Test acc': accuracy})\n",
    "    return val_losses\n",
    "\n",
    "def save_model(model: nn.Module, save_path: str):\n",
    "    save_dir = os.path.dirname(save_path)\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    torch.save(model.state_dict(), save_path)\n",
    "\n",
    "def load_model(model: nn.Module, save_path: str):\n",
    "    return model.load_state_dict(torch.load(save_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build loaders\n",
    "(\n",
    "    train_dataset, val_dataset, test_dataset,\n",
    "    train_loader, val_loader, test_loader\n",
    ") = get_rotated_mnist_loaders()\n",
    "\n",
    "# MODEL MATRIX TO TRAIN\n",
    "# param_factors = [2, 4, 8, 12, 16, 24]\n",
    "param_factors = [2, 4, 8, 24]\n",
    "model_names = ['ref_sqA', 'ref_sqB', 'goe_oracle', 'goe_random', 'goe_latent']\n",
    "num_runs_per_model = 3\n",
    "\n",
    "num_epochs = 2\n",
    "results = {}\n",
    "\n",
    "# routers\n",
    "oracle_router = MNISTOracleRouter()\n",
    "\n",
    "random_router = RandomBinaryTreeRouter(depth=3)\n",
    "random_router.compute_codebook(train_dataset)\n",
    "\n",
    "latent_router = LatentVariableRouter(depth=3)\n",
    "latent_router.compute_codebook(train_dataset)\n",
    "\n",
    "# init results\n",
    "for param_factor in param_factors:\n",
    "    for model_name in model_names:\n",
    "        results[(param_factor, model_name)] = []\n",
    "\n",
    "# train models\n",
    "for param_factor in param_factors:\n",
    "    for model_name in model_names:\n",
    "        for run in range(num_runs_per_model):\n",
    "            # Build architectures\n",
    "            modules_by_depth = build_modules(param_factor)\n",
    "            modules_by_depth_sqA = build_modules_sqA(param_factor)\n",
    "            \n",
    "            if model_name == 'ref_sqA':\n",
    "                model = ReferenceModel(modules_by_depth=modules_by_depth_sqA).to(device)\n",
    "            elif model_name == 'ref_sqB':\n",
    "                model = ReferenceModel(modules_by_depth=modules_by_depth).to(device)\n",
    "            elif model_name == 'goe_oracle':\n",
    "                model = BinaryTreeGoE(modules_by_depth = modules_by_depth, router=oracle_router).to(device)\n",
    "            elif model_name == 'goe_random':\n",
    "                model = BinaryTreeGoE(modules_by_depth = modules_by_depth, router=random_router).to(device)\n",
    "            elif model_name == 'goe_latent':\n",
    "                model = BinaryTreeGoE(modules_by_depth = modules_by_depth, router=latent_router).to(device)\n",
    "            else:\n",
    "                raise ValueError(f'Unknown model name: {model_name}')\n",
    "            \n",
    "            print(f'Training {model_name} at param_factor={param_factor}, run={run}')\n",
    "            \n",
    "            _ = training_loop(\n",
    "                model=model, \n",
    "                train_loader=train_loader, \n",
    "                val_loader=val_loader, \n",
    "                test_loader=test_loader, \n",
    "                num_epochs=num_epochs, \n",
    "                lr = 0.005, \n",
    "                epochs_per_accuracy=5\n",
    "            )\n",
    "            results[(param_factor, model_name)].append(model)\n",
    "            reference_save_path = f'checkpoints/rotated_mnist_oracle/param_{param_factor}_{model_name}_{run}.pt'\n",
    "            save_model(model, reference_save_path)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each experiment, compute accuracies, and flops\n",
    "accuracies = {}\n",
    "mean_accuracies = {}\n",
    "std_accuracies = {}\n",
    "flops = {}\n",
    "\n",
    "# example input\n",
    "image, rotation_label, digit_label = train_dataset[0]\n",
    "image = image.unsqueeze(0).to(device)\n",
    "rotation_label = torch.tensor(rotation_label).unsqueeze(0).to(device)\n",
    "router_metadata = {\n",
    "    \"rotation_labels\": rotation_label,\n",
    "}\n",
    "\n",
    "for k, models in tqdm(results.items()):\n",
    "    n_params, model_name = k\n",
    "    accuracies[k] = [get_accuracy(model, test_loader) for model in models]\n",
    "    mean_accuracies[k] = torch.mean(torch.Tensor(accuracies[k])).item()\n",
    "    std_accuracies[k] = torch.std(torch.Tensor(accuracies[k])).item()\n",
    "    \n",
    "    flops[k] = get_model_flops(models[0], (image, router_metadata))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile results by model name\n",
    "compiled_results = {}\n",
    "\n",
    "for (param_factor, model_name) in results:\n",
    "    if model_name not in compiled_results:\n",
    "        compiled_results[model_name] = {\n",
    "            'mean_accuracies': [],\n",
    "            'std_accuracies': [],\n",
    "            'flops': []\n",
    "        }\n",
    "    compiled_results[model_name]['mean_accuracies'].append(mean_accuracies[(param_factor, model_name)])\n",
    "    compiled_results[model_name]['std_accuracies'].append(std_accuracies[(param_factor, model_name)])\n",
    "    compiled_results[model_name]['flops'].append(flops[(param_factor, model_name)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save results to disk / load from disk\n",
    "torch.save(compiled_results, 'rotated_mnist_compiled_results.pt')\n",
    "compiled_results = torch.load('rotated_mnist_compiled_results.pt')\n",
    "\n",
    "# TODO: you can combine these dicts if people run different models \n",
    "# compiled_results.update(someone_elses_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_to_lablel = {\n",
    "    'ref_sqA': 'Ref SqA',\n",
    "    'ref_sqB': 'Ref SqB',\n",
    "    'goe_oracle': 'GoE Oracle',\n",
    "    'goe_random': 'GoE Random',\n",
    "    'goe_latent': 'GoE Latent'\n",
    "}\n",
    "# dont_plot = ['ref_sqA']\n",
    "dont_plot = []\n",
    "\n",
    "for model_name, statistics in compiled_results.items():\n",
    "    if model_name in dont_plot:\n",
    "        continue\n",
    "    mean_accuracies = statistics['mean_accuracies']\n",
    "    std_accuracies = statistics['std_accuracies']\n",
    "    flops = statistics['flops']\n",
    "    plt.errorbar(flops, mean_accuracies, yerr=std_accuracies, label=model_name_to_lablel[model_name])\n",
    "plt.xlabel('FLOPS')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Rotated MNIST')\n",
    "plt.legend()\n",
    "plt.savefig('rotated_mnist_oracle.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vqvae",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
