{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a54f26da-24d9-4b49-971f-144a1a5accbe",
   "metadata": {},
   "source": [
    "# Experiment 1: Oracle vs Reference on RotatedMNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d6309b-f9a3-4221-a5a7-0d44cb59ab89",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c895433-2741-47a4-b08a-3d58e2e5c7bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Tuple, List, Type\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.functional import cross_entropy\n",
    "from torch.utils.data import random_split, DataLoader\n",
    "from tqdm import tqdm\n",
    "from torch.functional import F\n",
    "\n",
    "from src.reference import ReferenceModel\n",
    "from src.data import RotatedMNISTDataset, FixedSizeWrapper\n",
    "from src.binary_tree import MNISTOracleRouter, BinaryTreeGoE, LatentVariableRouter, RandomBinaryTreeRouter\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "498dcac2-9996-4f65-a46d-8057616103c8",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ed0efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_modules_sqA(param_factor: int) -> Tuple[Type[nn.Module], Type[nn.Module], Type[nn.Module]]:\n",
    "    \"\"\"\n",
    "    Architecture factory for Status Quo A Model for RotatedMNIST task which has the same number of *total* parameters as the Graph of Experts. Takes in hyperparameter param_factor, outputs list of module architectures which scale linearly in param_factor\n",
    "    \"\"\"\n",
    "    class LayerOne(nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "            self.conv = nn.Conv2d(1, param_factor, kernel_size=3, padding=1)\n",
    "    \n",
    "        def forward(self, x: torch.Tensor):\n",
    "            \"\"\"\n",
    "            Args:\n",
    "            - x: (batch_size, 1, 28, 28)\n",
    "            \"\"\"\n",
    "            return F.relu(F.max_pool2d(self.conv(x), 2))  # (batch_size, param_factor, 14, 14)\n",
    "    \n",
    "    class LayerTwo(nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "            self.conv = nn.Conv2d(param_factor, 2 * 2 * param_factor, kernel_size=3, padding=1)\n",
    "    \n",
    "        def forward(self, x: torch.Tensor):\n",
    "            \"\"\"\n",
    "            Args:\n",
    "            - x: (batch_size, 32, 14, 14)\n",
    "            \"\"\"\n",
    "            return F.relu(F.max_pool2d(self.conv(x), 2))  # (batch_size, 2 * 2 * param_factor, 7, 7)\n",
    "    \n",
    "    class LayerThree(nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "            self.fc1 = nn.Linear(2 * 2 * param_factor * 7 * 7, 2 * 4 * param_factor)\n",
    "            self.fc2 = nn.Linear(2 * 4 * param_factor, 10)\n",
    "    \n",
    "        def forward(self, x: torch.Tensor):\n",
    "            \"\"\"\n",
    "            Args:\n",
    "            - x: (batch_size, 128, 7, 7)\n",
    "            \"\"\"\n",
    "            x = x.view(-1, 2 * 2 * param_factor * 7 * 7)  # (batch_size, 2 * 2 * param_factor * 7 * 7)\n",
    "            x = F.relu(self.fc1(x))  # (batch_size, 2 * 4 * param_factor)\n",
    "            logits = self.fc2(x)  # (batch_size, 10)\n",
    "            return logits\n",
    "    return LayerOne, LayerTwo, LayerThree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ecab386-3eba-479c-b553-e1c70aaad711",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_modules(param_factor: int) -> Tuple[Type[nn.Module], Type[nn.Module], Type[nn.Module]]:\n",
    "    \"\"\"\n",
    "    Architecture factory for RotatedMNIST task. Takes in hyperparameter param_factor, outputs list of module architectures which scale linearly in param_factor\n",
    "    \"\"\"\n",
    "    class LayerOne(nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "            self.conv = nn.Conv2d(1, param_factor, kernel_size=3, padding=1)\n",
    "    \n",
    "        def forward(self, x: torch.Tensor):\n",
    "            \"\"\"\n",
    "            Args:\n",
    "            - x: (batch_size, 1, 28, 28)\n",
    "            \"\"\"\n",
    "            return F.relu(F.max_pool2d(self.conv(x), 2))  # (batch_size, param_factor, 14, 14)\n",
    "    \n",
    "    class LayerTwo(nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "            self.conv = nn.Conv2d(param_factor, 2 * param_factor, kernel_size=3, padding=1)\n",
    "    \n",
    "        def forward(self, x: torch.Tensor):\n",
    "            \"\"\"\n",
    "            Args:\n",
    "            - x: (batch_size, 32, 14, 14)\n",
    "            \"\"\"\n",
    "            return F.relu(F.max_pool2d(self.conv(x), 2))  # (batch_size, 2 * param_factor, 7, 7)\n",
    "    \n",
    "    class LayerThree(nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "            self.fc1 = nn.Linear(2 * param_factor * 7 * 7, 4 * param_factor)\n",
    "            self.fc2 = nn.Linear(4 * param_factor, 10)\n",
    "    \n",
    "        def forward(self, x: torch.Tensor):\n",
    "            \"\"\"\n",
    "            Args:\n",
    "            - x: (batch_size, 64, 7, 7)\n",
    "            \"\"\"\n",
    "            x = x.view(-1, 2 * param_factor * 7 * 7)  # (batch_size, 2 * param_factor * 7 * 7)\n",
    "            x = F.relu(self.fc1(x))  # (batch_size, 4 * param_factor)\n",
    "            logits = self.fc2(x)  # (batch_size, 10)\n",
    "            return logits\n",
    "    return LayerOne, LayerTwo, LayerThree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b2dc82-0caf-4a83-afb2-1ce0a4413b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_train_epoch(model, loader, optimizer, epoch) -> float:\n",
    "    model.train()\n",
    "    epoch_losses = []\n",
    "    for idx, (images, rotation_labels, digit_labels) in tqdm(enumerate(loader)):\n",
    "        images = images.to(device)\n",
    "        rotation_labels = rotation_labels.to(device)\n",
    "        digit_labels = digit_labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(images, rotation_labels=rotation_labels) # (bs, num_digit_classes)\n",
    "        loss = cross_entropy(logits, digit_labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_losses.append(loss.item())\n",
    "    epoch_loss = torch.mean(torch.Tensor(epoch_losses)).item()\n",
    "    print(f'Train epoch {epoch} loss: {epoch_loss:.4f}')\n",
    "    return epoch_loss\n",
    "\n",
    "def do_val_epoch(model, loader, epoch) -> float:\n",
    "    model.eval()\n",
    "    epoch_losses = []\n",
    "    for idx, (images, rotation_labels, digit_labels) in tqdm(enumerate(loader)):\n",
    "        images = images.to(device)\n",
    "        rotation_labels = rotation_labels.to(device)\n",
    "        digit_labels = digit_labels.to(device)\n",
    "\n",
    "        logits = model(images, rotation_labels=rotation_labels) # (bs, num_digit_classes)\n",
    "        loss = cross_entropy(logits, digit_labels)\n",
    "        epoch_losses.append(loss.item())\n",
    "    epoch_loss = torch.mean(torch.Tensor(epoch_losses)).item()\n",
    "    print(f'Val epoch {epoch} loss: {epoch_loss:.4f}') \n",
    "    return epoch_loss\n",
    "\n",
    "def get_accuracy(model, loader):\n",
    "    model.eval()\n",
    "    total_samples = 0\n",
    "    total_correct = 0\n",
    "    for idx, (images, rotation_labels, digit_labels) in tqdm(enumerate(loader)):\n",
    "        images = images.to(device)\n",
    "        rotation_labels = rotation_labels.to(device)\n",
    "        digit_labels = digit_labels.to(device) # (bs,)\n",
    "\n",
    "        logits = model(images, rotation_labels=rotation_labels) # (bs, num_digit_classes)\n",
    "        prediction = torch.argmax(logits, dim=1) # (bs,)\n",
    "        total_samples += prediction.shape[0]\n",
    "        total_correct += torch.sum(prediction == digit_labels).item()\n",
    "    accuracy = total_correct / total_samples * 100\n",
    "    return accuracy\n",
    "\n",
    "def get_rotated_mnist_loaders(downsample_factor: int = 20) -> Tuple[DataLoader, DataLoader, DataLoader]:\n",
    "    # Initialize dataset\n",
    "    dataset = RotatedMNISTDataset()\n",
    "    \n",
    "    # Assuming `dataset` is your PyTorch Dataset\n",
    "    dataset_size = len(dataset)\n",
    "    train_size = int(0.7 * dataset_size)\n",
    "    val_size = int(0.2 * dataset_size)\n",
    "    test_size = dataset_size - train_size - val_size\n",
    "    \n",
    "    train_dataset, val_dataset, test_dataset = random_split(\n",
    "        dataset, [train_size, val_size, test_size], \n",
    "        generator=torch.Generator().manual_seed(40)\n",
    "    )\n",
    "    \n",
    "    train_dataset = FixedSizeWrapper(dataset = train_dataset, size = train_size // downsample_factor)\n",
    "    val_dataset = FixedSizeWrapper(dataset = val_dataset, size = val_size // downsample_factor)\n",
    "    test_dataset = FixedSizeWrapper(dataset = test_dataset, size = test_size // downsample_factor)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=False)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "    return train_dataset, val_dataset, test_dataset, train_loader, val_loader, test_loader\n",
    "\n",
    "def training_loop(model: nn.Module, train_loader: DataLoader, val_loader: DataLoader, test_loader: DataLoader, num_epochs: int = 100, lr: float = 0.005, epochs_per_accuracy: int = 5) -> List[float]:\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    val_losses = []\n",
    "    for epoch in range(num_epochs):\n",
    "        do_train_epoch(model, train_loader, optimizer, epoch)\n",
    "        val_loss = do_val_epoch(model, val_loader, epoch)\n",
    "        val_losses.append(val_loss)\n",
    "        if epoch % epochs_per_accuracy == 0:\n",
    "            accuracy = get_accuracy(model, test_loader)\n",
    "            print(f'Test accuracy: {accuracy:.3f}%')\n",
    "    return val_losses\n",
    "\n",
    "def save_model(model: nn.Module, save_path: str):\n",
    "    save_dir = os.path.dirname(save_path)\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    torch.save(model.state_dict(), save_path)\n",
    "\n",
    "def load_model(model: nn.Module, save_path: str):\n",
    "    return model.load_state_dict(torch.load(save_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49354bad-0d55-480e-839c-720b01533ff1",
   "metadata": {},
   "source": [
    "# Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed2895c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build loaders\n",
    "(\n",
    "    train_dataset, val_dataset, test_dataset,\n",
    "    train_loader, val_loader, test_loader\n",
    ") = get_rotated_mnist_loaders()\n",
    "# param_factors = [2, 4, 8, 12, 16, 24] # The LOST numbers :D\n",
    "param_factors = [2]\n",
    "num_epochs = 1\n",
    "reference_models = {}\n",
    "goe_oracle_models = {}\n",
    "\n",
    "print(f\"Number of training samples: {len(train_dataset)}\")\n",
    "# print(train_dataset[-1])\n",
    "# from tqdm import tqdm\n",
    "for el in tqdm(train_dataset):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f4b8f66-9109-4f64-99b9-a99d01ef02dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build loaders\n",
    "(\n",
    "    train_dataset, val_dataset, test_dataset,\n",
    "    train_loader, val_loader, test_loader\n",
    ") = get_rotated_mnist_loaders()\n",
    "param_factors = [2, 4, 8, 12, 16, 24] # The LOST numbers :D\n",
    "# param_factors = [2]\n",
    "num_epochs = 100\n",
    "reference_models = {}\n",
    "reference_sqA_models = {}\n",
    "goe_oracle_models = {}\n",
    "goe_random_models = {}\n",
    "goe_latent_models = {}\n",
    "\n",
    "# routers\n",
    "oracle_router = MNISTOracleRouter()\n",
    "\n",
    "random_router = RandomBinaryTreeRouter(depth=3)\n",
    "random_router.compute_codebook(train_dataset)\n",
    "\n",
    "latent_router = LatentVariableRouter(depth=3)\n",
    "latent_router.compute_codebook(train_dataset)\n",
    "\n",
    "# train models\n",
    "for param_factor in param_factors:\n",
    "    # Build architectures\n",
    "    modules_by_depth = build_modules(param_factor)\n",
    "    modules_by_depth_sqA = build_modules_sqA(param_factor)\n",
    "    # TODO: Compute FLOPS, etc. \n",
    "    \n",
    "    # Train reference model + save    \n",
    "    print(f'Training reference at param_factor={param_factor}')\n",
    "    reference_model = ReferenceModel(modules_by_depth=modules_by_depth).to(device)\n",
    "    _ = training_loop(\n",
    "        model=reference_model, \n",
    "        train_loader=train_loader, \n",
    "        val_loader=val_loader, \n",
    "        test_loader=test_loader, \n",
    "        num_epochs=num_epochs, \n",
    "        lr = 0.005, \n",
    "        epochs_per_accuracy=5\n",
    "    )\n",
    "    reference_models[param_factor] = reference_model\n",
    "    reference_save_path = f'checkpoints/rotated_mnist_oracle/param_{param_factor}_reference.pt'\n",
    "    save_model(reference_model, reference_save_path)\n",
    "\n",
    "    print(f'Training SQ-A Reference Model at param_factor={param_factor}')\n",
    "    reference_sqA_model = ReferenceModel(modules_by_depth=modules_by_depth_sqA).to(device)\n",
    "    _ = training_loop(\n",
    "        model=reference_model, \n",
    "        train_loader=train_loader, \n",
    "        val_loader=val_loader, \n",
    "        test_loader=test_loader, \n",
    "        num_epochs=num_epochs, \n",
    "        lr = 0.005, \n",
    "        epochs_per_accuracy=5\n",
    "    )\n",
    "    reference_sqA_models[param_factor] = reference_sqA_model\n",
    "    reference_sqA_save_path = f'checkpoints/rotated_mnist_oracle/param_{param_factor}_reference_sqA.pt'\n",
    "    save_model(reference_sqA_model, reference_sqA_save_path)\n",
    "    \n",
    "    # Train GoE model + save\n",
    "    print(f'Training GoE at param_factor={param_factor}')\n",
    "    goe_oracle_model = BinaryTreeGoE(modules_by_depth = modules_by_depth, router=oracle_router).to(device)\n",
    "    val_losses = training_loop(\n",
    "        model=goe_oracle_model, \n",
    "        train_loader=train_loader, \n",
    "        val_loader=val_loader, \n",
    "        test_loader=test_loader, \n",
    "        num_epochs=num_epochs, \n",
    "        lr = 0.005, \n",
    "        epochs_per_accuracy=5\n",
    "    )\n",
    "    goe_oracle_models[param_factor] = goe_oracle_model\n",
    "    goe_save_path = f'checkpoints/rotated_mnist_oracle/param_{param_factor}_goe_oracle.pt'\n",
    "    save_model(goe_oracle_model, goe_save_path)\n",
    "    \n",
    "    # Train random GoE model + save\n",
    "    print(f'Training Random GoE at param_factor={param_factor}')\n",
    "    goe_random_model = BinaryTreeGoE(modules_by_depth = modules_by_depth, router=random_router).to(device)\n",
    "    val_losses = training_loop(\n",
    "        model=goe_random_model, \n",
    "        train_loader=train_loader, \n",
    "        val_loader=val_loader, \n",
    "        test_loader=test_loader, \n",
    "        num_epochs=num_epochs, \n",
    "        lr = 0.005, \n",
    "        epochs_per_accuracy=5\n",
    "    )\n",
    "    goe_random_models[param_factor] = goe_random_model\n",
    "    goe_save_path = f'checkpoints/rotated_mnist_oracle/param_{param_factor}_goe_random.pt'\n",
    "    save_model(goe_random_model, goe_save_path)\n",
    "    \n",
    "    # Train latent GoE model + save\n",
    "    print(f'Training Latent GoE at param_factor={param_factor}')\n",
    "    goe_latent_model = BinaryTreeGoE(modules_by_depth = modules_by_depth, router=latent_router).to(device)\n",
    "    val_losses = training_loop(\n",
    "        model=goe_latent_model, \n",
    "        train_loader=train_loader, \n",
    "        val_loader=val_loader, \n",
    "        test_loader=test_loader, \n",
    "        num_epochs=num_epochs, \n",
    "        lr = 0.005, \n",
    "        epochs_per_accuracy=5\n",
    "    )\n",
    "    goe_latent_models[param_factor] = goe_latent_model\n",
    "    goe_save_path = f'checkpoints/rotated_mnist_oracle/param_{param_factor}_goe_latent.pt'\n",
    "    save_model(goe_latent_model, goe_save_path)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "790c6c32-4b11-4004-8d9c-2f4b0b2c2977",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph results\n",
    "reference_accuracies = [get_accuracy(model, test_loader) for (_, model) in reference_models.items()]\n",
    "goe_oracle_accuracies = [get_accuracy(model, test_loader) for (_, model) in goe_oracle_models.items()]\n",
    "goe_random_accuracies = [get_accuracy(model, test_loader) for (_, model) in goe_random_models.items()]\n",
    "goe_latent_accuracies = [get_accuracy(model, test_loader) for (_, model) in goe_latent_models.items()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6fd7505",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(reference_accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc42cf9-cf21-4412-82ac-d097f6883c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(param_factors, reference_accuracies, label=\"Reference\")\n",
    "plt.plot(param_factors, goe_oracle_accuracies, label=\"GoE Oracle\")\n",
    "plt.plot(param_factors, goe_random_accuracies, label=\"GoE Random\")\n",
    "plt.plot(param_factors, goe_latent_accuracies, label=\"GoE Latent\")\n",
    "plt.title(\"Reference vs GoE # Parameters vs Accuracy\")\n",
    "plt.legend()\n",
    "# plt.show()\n",
    "plt.savefig('rotated_mnist_oracle.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d513d8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(reference_accuracies, param_factors, label=\"Reference\")\n",
    "plt.plot(goe_oracle_accuracies, param_factors, label=\"GoE\")\n",
    "plt.plot(goe_random_accuracies, param_factors, label=\"GoE Random\")\n",
    "plt.plot(goe_latent_accuracies, param_factors, label=\"GoE Latent\")\n",
    "plt.title(\"Reference vs GoE # Parameters vs Accuracy\")\n",
    "plt.legend()\n",
    "# plt.show()\n",
    "plt.savefig('rotated_mnist_oracle_T.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vqvae",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
